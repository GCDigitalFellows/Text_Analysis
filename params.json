{"name":"Gcdrb text analysis","tagline":"Workshop Outline and Notes for Text Analysis with NLTK","body":"# NLTK Tutorial\r\n\r\nMichelle Johnson-McSweeney\r\nPatrick Smyth\r\nJanuary 21, 2016\r\nGC Digital Research Bootcamp\r\n\r\nMany of the exercises are from the [NLTK Book](http://www.nltk.org/book/), which is a great introduction to text analysis\r\n\r\nMajor steps in doing language analysis\r\n1. Getting the data\r\n2. Turning the data into numbers\r\n3. Analyzing the data \r\n\r\nThis session focuses on 2. Turning the data into numbers.\r\nThere are lots of ways to do this, both programmatic and non-programmatic.\r\n\r\nQ--> What parts of language (spoken, written, texted) can you count? What numbers can you come up with beyond counting?\r\n\r\nBefore we cover this, we have to talk about data types\r\n* strings - anything can be a string. Think of this like a sequence of characters. Python can't look inside unless you tell it to. \r\n```\r\nmy_string = \"I am a Digital Researcher!\"\r\n```\r\n* lists - just as it says, this is a list of items. Python can see the things in the list, but not inside the things in the list. Can add things to the list with '+' - so long as it is also a list. Otherwise, \"append\"\r\n\r\n```\r\nmy_list = [] (makes an empty list)\r\nlove_list = ['love', 'hope', 'joy', 'amor']\r\n```\r\n\r\n* dictionaries - key/value pair. Useful for keeping tallies of things: i.e., number of times a word appears i.e., {a:200, b:15, c:135 ... }\r\n\r\n```\r\nmy_dict = {} (makes an empty dictionary)\r\n```\r\n\r\n## All the things you can look at with text analysis\r\n1. Text Editor Functions\r\na. Word counts\r\nb. Word-in-context (text wrangler & sublime)\r\n\r\n### Let's get started with Python\r\nNLTK Functions\r\n```\r\nimport nltk\r\nfrom nltk.book import *\r\n```\r\n\r\ni. Concordance - shows the context that the word occurs in\r\n```\r\ntext1.concordance('WORD')\r\n```\r\nii. Similar - shows words that appear near similar words **very illuminating if looking at 2 different texts - how does one person use \"love\" versus another\"?\r\n```\r\ntext1.similar('WORD')\r\n```\r\niii. Common contexts - used to compare two words - in what environments do they both occur?  \r\nQ--> The syntax changed here - we had to use brackets when we gave it the words to look for - WHY?!?\r\n```\r\ntext1.common_contexts([WORD, WORD]) \r\n```\r\niv. lexical dispersion plot - good for plotting word use over time or throughout the course of a book\r\n```\r\ntext1.dispersion_plot(['hope','justice','freedom')]\r\n```\r\n** A pop-up will appear with the dispersion plot. You can save this if you want. This MUST BE CLOSED TO MOVE ON\r\n\r\nv. Count a specific word - how many times does this sequence of characters occur in my document?\r\n```\r\ntext1.count(\"love\")\r\n```\r\nvi. Count tokens - tokens are sequences (i.e., words and punctuation, so \"love\", \"bowie\", \"Bowie\", \"!\" and \":)\" are tokens)\r\n```\r\nlen(text1)\r\n```\r\nvi. Count unique words - first have to make a set that groups all the \"words\" together (numbers, punctuation sequences, etc.) - this groups together types. Token = instance, Type = more general (\"bowie\" and \"Bowie\" are different types)\r\n\r\n1. make a set of words \r\n```\r\nset(text1) \r\n```\r\n(you might want to sort this set if you want to organize it)\r\n```\r\nsorted(set(text1)\r\n```\r\n2. count how many items are in that set \r\n```\r\nlen(set(text1)))\r\n```\r\nvii. Lexical Density! We have a number!! The number of unique tokens divided by the total number of words. This is a descriptive measure of language register.\r\n```\r\nlen(set(text1))/len(text1)\r\n```\r\nviii. Frequency Distribution! We have another number!!\r\n1. first make an object that Python can look at:\r\n```\r\nmy_dist = FreqDist(text1)\r\n```\r\nI like to check if its there\r\n```\r\ntype(my_dist)\r\n```\r\n2. Tell python to plot this on a graph so we can inspect it\r\n```\r\nmy_dist.plot(50,cumulative=False)\r\n```\r\n3. Make a dictionary of the 100 most common words and how often they occur\r\n```\r\nmy_dist.most_common(100) - gives the top 100 words and their numbers\r\n```\r\nWe could go on and on with things you can do. \r\n\r\nc. Pythonic non-NLTK Functions\r\n\r\nLet's say I am interested in words that I think are related to love and I want to check if they occur in the \r\nii. Intro sentiment analysis - the love text example \r\n```\r\nlove_list = []\r\n```\r\n1. make a list of words you want to use \r\n```\r\nlove_words = ['love', 'joy', 'hope', 'amor']\r\n```\r\n2. loop through all the words in your corpus\r\n```\r\nfor word in love_words:\r\n    if word in text1:\r\n        love_list.append(word)\r\n    else:\r\n        pass\r\n```\r\n3. Check to see what you got!\r\n```\r\nprint(love_list)\r\n```\r\nd. Regular Expressions - used for pattern matching and data cleaning. It is worth mention, but not going to discuss.\r\n\r\n\r\n3. Your own text\r\na. There are many [corpus readers](http://www.nltk.org/howto/corpus.html)s to make this easier, but we are going to go step-by-step to develop an understanding of what is going on.\r\n\r\nb. Making an NLTK Text\r\ni. From the internet - We're going to use [Don Quixote](http://www.gutenberg.org/cache/epub/996/pg996.txt) from the Gutenberg Project\r\n```\r\nfrom urllib.request import urlopen\r\nurl = u\"http://www.gutenberg.org/cache/epub/996/pg996.txt\"\r\nraw = urlopen(url).read()\r\nraw = raw.decode(\"utf-8\", \"ignore\")\r\n```\r\nThat is all that you SHOULD have to do, but if you just do this, you will get an ASCII Error. Python can't deal with non-English characters, so this is a little piece of code to fix it. For more on that and how it relates to Python, visit the [Python docs](https://docs.python.org/3/howto/unicode.html). For the purpose of this workshop, put in this last piece of code.\r\nCheck to be sure it's right\r\n```\r\ntype(raw)\r\n```\r\nii. Now need to break this giant string into a list of things we recognize - tokens (words, punctuation, etc.)\r\n```\r\ntokens = nltk.word_tokenize(raw)  \r\n```\r\niii. this makes a list of tokens - let's check to make sure its correct \r\n```\r\ntokens[:10]\r\n```\r\niv. This is enough to read it in to make your own files, but to use the NLTK features, need to make it an NLTK Text\r\n```\r\ndq_text = nltk.Text(tokens)\r\n```\r\nv. Check to be sure it worked\r\n```\r\ntype(dq_text)\r\n``` \r\nvi. Now let's use our favorite new techniques.\r\n\r\nvii. From your own files exactly the same, just have to read the file in.\r\n```\r\ninfile = open(\"PATH\", \"r\")\r\nmy_file = infile.read()\r\n```\r\nFrom here, the whole word is open to you... For more information and ways to work with text, refer to the [NLTK Book](http://www.nltk.org/book/)! \r\nIf you start working in Python to do text analysis, come to the PUG or Office Hours - there is so much more to learn!!","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}