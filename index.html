<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>GCDRB text analysis by michellejm</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Gcdrb text analysis</h1>
        <p>Workshop Outline and Notes for Text Analysis with NLTK</p>

        <p class="view"><a href="https://github.com/michellejm/GCDRB_Text_Analysis">View the Project on GitHub <small>michellejm/GCDRB_Text_Analysis</small></a></p>


        <ul>
          <li><a href="https://github.com/michellejm/GCDRB_Text_Analysis/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/michellejm/GCDRB_Text_Analysis/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/michellejm/GCDRB_Text_Analysis">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="nltk-tutorial" class="anchor" href="#nltk-tutorial" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>NLTK Tutorial</h1>


<p>Many of the exercises are from the <a href="http://www.nltk.org/book/">NLTK Book</a>, which is a great introduction to text analysis</p>

<p>Major steps in doing language analysis
<ol>
<li> Getting the data</li>
<li> Turning the data into numbers</li>
<li> Analyzing the data </li>
</ol>

<p>This session focuses on Turning the data into numbers.
There are lots of ways to do this, both programmatic and non-programmatic.</p>

<p>Q--&gt; What parts of language (spoken, written, texted) can you count? What numbers can you come up with beyond counting?</p>

<p>Before we cover this, we have to talk about data types</p>

<ul>
<li>strings - anything can be a string. Think of this like a sequence of characters. Python can't look inside unless you tell it to. </li>
</ul>

<pre><code>my_string = "I am a Digital Researcher!"
</code></pre>

<ul>
<li>lists - just as it says, this is a list of items. Python can see the things in the list, but not inside the things in the list. Can add things to the list with '+' - so long as it is also a list. Otherwise, "append"</li>
</ul>

<pre><code>my_list = [] (makes an empty list)
love_list = ['love', 'hope', 'joy', 'amor']
</code></pre>

<ul>
<li>dictionaries - key/value pair. Useful for keeping tallies of things: i.e., number of times a word appears i.e., {a:200, b:15, c:135 ... }</li>
</ul>

<pre><code>my_dict = {} (makes an empty dictionary)
</code></pre>

<h2>
<a id="all-the-things-you-can-look-at-with-text-analysis" class="anchor" href="#all-the-things-you-can-look-at-with-text-analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>All the things you can look at with text analysis</h2>

<ol>
<li>Text Editor Functions
a. Word counts
b. Word-in-context (text wrangler &amp; sublime)</li>
</ol>

<h3>
<a id="lets-get-started-with-python" class="anchor" href="#lets-get-started-with-python" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Let's get started with Python</h3>

<p>NLTK Functions</p>
<p> Very Helpful Cheatsheet<a href="https://blogs.princeton.edu/etc/files/2014/03/Text-Analysis-with-NLTK-Cheatsheet.pdf">Cheatsheet</a> from Princeton</p>

<pre><code>import nltk
from nltk.book import *
</code></pre>

<p>i. Concordance - shows the context that the word occurs in</p>

<pre><code>text1.concordance('WORD')
</code></pre>

<p>ii. Similar - shows words that appear near similar words **very illuminating if looking at 2 different texts - how does one person use "love" versus another"?</p>

<pre><code>text1.similar('WORD')
</code></pre>

<p>iii. Common contexts - used to compare two words - in what environments do they both occur?<br>
Q--&gt; The syntax changed here - we had to use brackets when we gave it the words to look for - WHY?!?</p>

<pre><code>text1.common_contexts([WORD, WORD]) 
</code></pre>

<p>iv. lexical dispersion plot - good for plotting word use over time or throughout the course of a book</p>

<pre><code>text1.dispersion_plot(['hope','justice','freedom')]
</code></pre>

<p>** A pop-up will appear with the dispersion plot. You can save this if you want. This MUST BE CLOSED TO MOVE ON</p>

<p>v. Count a specific word - how many times does this sequence of characters occur in my document?</p>

<pre><code>text1.count("love")
</code></pre>

<p>vi. Count tokens - tokens are sequences (i.e., words and punctuation, so "love", "bowie", "Bowie", "!" and ":)" are tokens)</p>

<pre><code>len(text1)
</code></pre>

<p>vi. Count unique words - first have to make a set that groups all the "words" together (numbers, punctuation sequences, etc.) - this groups together types. Token = instance, Type = more general ("bowie" and "Bowie" are different types)</p>

<ol>
<li>make a set of words </li>
</ol>

<pre><code>set(text1) 
</code></pre>

<p>(you might want to sort this set if you want to organize it)</p>

<pre><code>sorted(set(text1)
</code></pre>

<ol>
<li>count how many items are in that set </li>
</ol>

<pre><code>len(set(text1)))
</code></pre>

<p>vii. Lexical Density! We have a number!! The number of unique tokens divided by the total number of words. This is a descriptive measure of language register.</p>

<pre><code>len(set(text1))/len(text1)
</code></pre>

<p>viii. Frequency Distribution! We have another number!!
1. first make an object that Python can look at:</p>

<pre><code>my_dist = FreqDist(text1)
</code></pre>

<p>I like to check if its there</p>

<pre><code>type(my_dist)
</code></pre>

<ol>
<li>Tell python to plot this on a graph so we can inspect it</li>
</ol>

<pre><code>my_dist.plot(50,cumulative=False)
</code></pre>

<ol>
<li>Make a dictionary of the 100 most common words and how often they occur</li>
</ol>

<pre><code>my_dist.most_common(100) - gives the top 100 words and their numbers
</code></pre>

<p>We could go on and on with things you can do. </p>

<p>c. Pythonic non-NLTK Functions</p>

<p>Let's say I am interested in words that I think are related to love and I want to check if they occur in the 
ii. Intro sentiment analysis - the love text example </p>

<pre><code>love_list = []
</code></pre>

<ol>
<li>make a list of words you want to use </li>
</ol>

<pre><code>love_words = ['love', 'joy', 'hope', 'amor']
</code></pre>

<ol>
<li>loop through all the words in your corpus</li>
</ol>

<pre><code>for word in love_words:
    if word in text1:
        love_list.append(word)
    else:
        pass
</code></pre>

<ol>
<li>Check to see what you got!</li>
</ol>

<pre><code>print(love_list)
</code></pre>

<p>d. Regular Expressions - used for pattern matching and data cleaning. It is worth mention, but not going to discuss.</p>

<ol>
<li>Your own text
a. There are many <a href="http://www.nltk.org/howto/corpus.html">corpus readers</a> to make this easier, but we are going to go step-by-step to develop an understanding of what is going on.</li>
</ol>

<p>b. Making an NLTK Text
i. From the internet - We're going to use <a href="http://www.gutenberg.org/cache/epub/996/pg996.txt">Don Quixote</a> from the Gutenberg Project</p>

<pre><code>from urllib.request import urlopen
url = u"http://www.gutenberg.org/cache/epub/996/pg996.txt"
raw = urlopen(url).read()
raw = raw.decode("utf-8", "ignore")
</code></pre>

<p>That is all that you SHOULD have to do, but if you just do this, you will get an ASCII Error. Python can't deal with non-English characters, so this is a little piece of code to fix it. For more on that and how it relates to Python, visit the <a href="https://docs.python.org/3/howto/unicode.html">Python docs</a>. For the purpose of this workshop, put in this last piece of code.
Check to be sure it's right</p>

<pre><code>type(raw)
</code></pre>

<p>ii. Now need to break this giant string into a list of things we recognize - tokens (words, punctuation, etc.)</p>

<pre><code>tokens = nltk.word_tokenize(raw)  
</code></pre>

<p>iii. this makes a list of tokens - let's check to make sure its correct </p>

<pre><code>tokens[:10]
</code></pre>

<p>iv. This is enough to read it in to make your own files, but to use the NLTK features, need to make it an NLTK Text</p>

<pre><code>dq_text = nltk.Text(tokens)
</code></pre>

<p>v. Check to be sure it worked</p>

<pre><code>type(dq_text)
</code></pre>

<p>vi. Now let's use our favorite new techniques.</p>

<p>vii. From your own files exactly the same, just have to read the file in.</p>

<pre><code>infile = open("PATH", "r")
my_file = infile.read()
</code></pre>

<p>From here, the whole word is open to you... For more information and ways to work with text, refer to the <a href="http://www.nltk.org/book/">NLTK Book</a>! 
If you start working in Python to do text analysis, come to the PUG or Office Hours - there is so much more to learn!!</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/michellejm">michellejm</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
